{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccf0bf87",
   "metadata": {},
   "source": [
    "# In Depth Analysis of Linear Regression\n",
    "- **ML Overview**\n",
    "    - Example, Algorithms vs Model\n",
    "- **Supervised Learning**\n",
    "    - Definition, Examples\n",
    "- **Supervised Learning Setup**\n",
    "    - Nomenclature, Formulation(`Regression` & `Classification`),  Example,  Learning,  Hypothesis Class.\n",
    "    - Performance Evaluation\n",
    "        - Loss Function, 0/1 Loss Function, Squared Loss, Root Mean squared error, Absolute Loss\n",
    "    - Generalization: The Train-Test Split, Generalization loss.\n",
    "    \n",
    "[Linear Regression](#Linear-Regression)\n",
    "\n",
    "- **Single Feature**\n",
    "- **Multiple Feature**\n",
    "- **Model Formulation and Setup**\n",
    "- **Loss Function**\n",
    "   - How to solve?\n",
    "   - Reformulation\n",
    "   - Consequently\n",
    "- **Solve Optimization Problem (Analytical Solution employing Calculus)**\n",
    "- **Model Evaluation Techniques**\n",
    "- **Polynomial Regression**\n",
    "- **How to Handle Overfitting?**\n",
    "- **Regularization (Ridge Regression and Lasso Regression)**\n",
    "- **Gradient Descent Algorithm**\n",
    "   - Formulation\n",
    "   - Algorithm\n",
    "   - Types \n",
    "- **Linear Regression Implementation in Python**\n",
    "- **Linear Regression Implementation using sklearn**\n",
    "- **Interview Questions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1cc6a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "40f24ee6",
   "metadata": {},
   "source": [
    "<h2 align='center' > ML Overview</h2> \n",
    "\n",
    "### What is Machine Learning?\n",
    "- Automating the process of automation\n",
    "- Getting computers to program themselves\n",
    "\n",
    "<figure>\n",
    "<img src=\"images/p1.png\" width=300px height=300px>\n",
    "<figcaption align = \"center\" ><b> Given examples(Training data), make a machine learn system behavior or discover patterns. </b> </figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "> **A simple definition of machine learning is the ability of a computer or machine to improve its performance on a specific task through experience. It involves training a model on a data set, and then using that model to make predictions or take actions based on new, unseen data.**\n",
    "\n",
    "> **In other words, machine learning is a way for computers to learn and make decisions on their own, without being explicitly programmed to perform a specific task.**\n",
    "-----\n",
    "\n",
    "<img src=\"images/p2.png\">\n",
    "\n",
    "&ensp;\n",
    "\n",
    "**Note**: An algorithm is a set of steps that a model follows to learn from data and make predictions. A model, on the other hand, is a representation of the relationships between the input data and the output predictions. It is trained on a dataset and is able to make predictions on new data based on what it has learned from the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c033440f",
   "metadata": {},
   "source": [
    "### Rules vs. Learning\n",
    "- `Rules-based` approaches involve following a predetermined set of rules or instructions to solve a problem or make a decision. These rules are typically established in advance and are followed regardless of the specific circumstances or context.\n",
    "\n",
    "- On the other hand, `learning-based` approaches involve using data and experience to improve decision-making over time. In the context of machine learning, this involves training a model on a dataset, allowing the model to learn patterns and relationships in the data, and using this learned knowledge to make predictions on new data.\n",
    "\n",
    "- Both rules-based and learning-based approaches have their own strengths and weaknesses, and the most appropriate approach will depend on the specific problem or task at hand. Rules-based approaches are often simpler and easier to implement, but they may not be able to adapt to changing circumstances or new data as well as learning-based approaches. Learning-based approaches, on the other hand, may be more complex and require more data and computation, but they have the ability to improve over time and adapt to new situations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5940a3a",
   "metadata": {},
   "source": [
    "<h2 align=\"center\">Supervised Learning</h2> \n",
    "\n",
    "**Predicting the labels for unseen data based on labelled\n",
    "instances.** \n",
    "\n",
    "<img src=\"images/p4.png\" height=500px width=500px align='center'>\n",
    "\n",
    "- Each column is a feature and adds one dimension to the data\n",
    "- Number of columns define total number of features and hence data dimensionality.\n",
    "- `Inputs`: referred to as Features.\n",
    "- `Output`: referred to as Label.\n",
    "- `Training data`: (input, output) for which the output is known and is used for training a model by ML algorithm.\n",
    "- `A Loss, an objective or a cost function`: determines how well a trined model approximates the training data\n",
    "- `Test data`: (input, output) for which the output is known and is used for the evaluation of the performance of the trained model\n",
    "\n",
    "#### A Recipe for Applying Supervised Learning\n",
    "\n",
    "To apply supervised learning, we define a dataset and a learning algorithm.\n",
    "\n",
    "$$ \\text{Dataset} + \\text{Learning Algorithm} \\to \\text{Predictive Model} $$\n",
    "\n",
    "The output is a predictive model that maps inputs to targets. For instance, it can predict targets on new inputs.\n",
    "\n",
    "-----\n",
    "The learning algorithm would receive a set of inputs along with the corresponding correct outputs to train a model.\n",
    "\n",
    "<img src=\"images/p5.png\" height=600px width=600px align=\"center\">\n",
    "&ensp;\n",
    "\n",
    "<img src=\"images/p8.png\" height=600px width=600px align=\"center\">\n",
    "\n",
    "\n",
    "### Algorithms vs Model\n",
    "- `Linear regression` algorithm produces a model, that is, a vector of values of the coefficients of the model.\n",
    "- `Decision tree` algorithm produces a model comprised of a tree of if-then statements with specific values.\n",
    "- `Neural network` along with backpropagation + gradient descent: produces a model comprised of a trained (weights assigned) neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e25535",
   "metadata": {},
   "source": [
    "<h2 align=\"center\">Supervised Learning Setup</h2>\n",
    "\n",
    "##### A Supervised Learning Dataset: Notation\n",
    "\n",
    "Using the adopted notation, we can formalize the supervised machine learning setup. We represent the entire training data as \n",
    "$$\\mathcal{D} = \\{(x^{(i)}, y^{(i)}) \\mid i = 1,2,...,n\\} \\in X^{R} \\times Y$$\n",
    "\n",
    "Where\n",
    "- D is a dataset\n",
    "- X is the d-dimensional feature space($R^d$)\n",
    "- $x_i$ is the input vector of the <i>ith</i> sample\n",
    "- Y is the label space\n",
    "\n",
    "Each $x^{(i)}$ denotes an input (e.g., the measurements for patient $i$), and each $y^{(i)} \\in \\mathcal{Y}$ is a target (e.g., the Heart Disease). \n",
    "\n",
    "<b><u>Regression</u></b>:    $\\mathcal{y} = R (prediction-on-continuous-scale)$\n",
    "\n",
    "<b><u>Classification</u></b>:    $\\mathcal{y} = {{0,1}} \\;\\;or \\;\\;{-1,1} \\;\\; or \\;\\; 1,2 \\;\\; binary-classification $\n",
    "\n",
    "$\\mathcal{y} = {1,2,3......,M} \\;\\;\\; M-class-Classification$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Together, $(x^{(i)}, y^{(i)})$ form a *training example*.\n",
    "&emsp;\n",
    "<h3 align='center'>Example</h3>\n",
    "<img src=\"images/9.png\" height=500px width=500px >\n",
    "\n",
    "&emsp;\n",
    "\n",
    "<h3 align='center'>Learning </h3>\n",
    "\n",
    "We want to develop a model that can predict the label for the input for which label is **unknown.**\n",
    "\n",
    "We assume that the data points $(x_i,y_i)$ are drawn from some **unknown** distribution $P(X,Y)$.\n",
    "<img src=\"images/10.png\" align='center'>\n",
    "\n",
    "Our goal is to learn the machine (model, function or hypothesis) $h\\in H$, such that for a new pair/instance $(x,y)P$ , we can use *h* to obtain\n",
    "$$h(x) =y$$\n",
    "\n",
    "with high probability or \n",
    "$$h(x)\\approx y$$\n",
    "\n",
    "in some optimal sense.\n",
    "\n",
    "**Note:** Here, we do not know exact distribution but using **ML**, we try to estimate that distribution. Here, $h$ is our hypothesis, this is basically returns a mapping between input and output. $h$ is a machine. It is a sample from Hypothesis $H$. Here, we want to learn a function from dataset which takes input and produces its output.\n",
    "\n",
    "<h3 align='center'> Model/Function/Hypothesis: Notation</h3>\n",
    "\n",
    "We'll say that a model is a function\n",
    "$$ f : \\mathcal{X} \\to \\mathcal{Y} $$\n",
    "that maps inputs $x \\in \\mathcal{X}$ to targets $y \\in \\mathcal{Y}$.\n",
    "\n",
    "Often, models have *parameters* $\\theta \\in \\Theta$ living in a set $\\Theta$. We will then write the model as\n",
    "$$ f_\\theta : \\mathcal{X} \\to \\mathcal{Y} $$\n",
    "to denote that it's parametrized by $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e91caaf",
   "metadata": {},
   "source": [
    "<h3 align=\"center\">Feature Space</h3>  \n",
    "\n",
    "**Definition:** A feature space is a set of mathematical features or variables that are used to represent the data. It is a multi-dimensional space where each dimension corresponds to a feature or variable, and each data point is represented as a point in this space.\n",
    "\n",
    "**Example:** If you have a dataset containing three features (age, height, and weight), the feature space would be a three-dimensional space where each data point is represented by its age, height, and weight.\n",
    "\n",
    "- Student data(e.g. for predicting grades), an input $x^{(i)} \\in \\mathcal{X}$ is a $d$-dimensional vector of the form\n",
    "$$ x^{(i)} = \\begin{bmatrix}\n",
    "x^{(i)}_1 \\\\\n",
    "x^{(i)}_2 \\\\\n",
    "\\vdots \\\\\n",
    "x^{(i)}_d\n",
    "\\end{bmatrix}$$\n",
    "- Where each $x_j^i \\;$ is the value of the $ith$ feature for student $j$.\n",
    "- Examples of features, $x^i$\n",
    "    - Scores in assignments, quizzes, exams\n",
    "    - Educational records, grades in previous courses\n",
    "    - Rankings of previous educational institutes\n",
    "    - Interaction with online tools? Missed instruments?\n",
    "- Small number of features and relatively low features with 0 values: **Dense vectors**.\n",
    "\n",
    "The set $\\mathcal{X}$ is called the feature space. Often, we have, $\\mathcal{X} = \\mathbb{R}^d$.\n",
    "\n",
    "**Attributes:** We refer to the numerical variables describing the patient as *attributes*. Examples of attributes include:\n",
    "* The age of a patient.\n",
    "* The patient's gender.\n",
    "* The patient's BMI.\n",
    "\n",
    "**Features:** Often, an input object has many attributes, and we want to use these attributes to define more complex descriptions of the input.\n",
    "\n",
    "* Is the patient old and a man? (Useful if old men are at risk).\n",
    "* Is the BMI above the obesity threshold?\n",
    "\n",
    "We call these custom attributes *features*.\n",
    "\n",
    "**Feature:** We may denote features via a function $\\phi : \\mathcal{X} \\to \\mathbb{R}^p$ that takes an input $x^{(i)} \\in \\mathcal{X}$ and outputs a $p$-dimensional vector\n",
    "$$ \\phi(x^{(i)}) = \\left[\\begin{array}{@{}c@{}}\n",
    "\\phi(x^{(i)})_1 \\\\\n",
    "\\phi(x^{(i)})_2 \\\\\n",
    "\\vdots \\\\\n",
    "\\phi(x^{(i)})_p\n",
    "\\end{array} \\right]$$\n",
    "We say that $\\phi(x^{(i)})$ is a *featurized* input, and each $\\phi(x^{(i)})_j$ is a *feature*.\n",
    "\n",
    "**Features vs Attributes**:\n",
    "In practice, the terms attribute and features are often used interchangeably. Most authors refer to $x^{(i)}$ as a vector of features. We will follow this convention and use the term \"attribute\" only when there is ambiguity between features and attributes. Features can be either discrete or continuous."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936edb6e",
   "metadata": {},
   "source": [
    "<h3 align='center'>Label Space</h3>\n",
    "Formally, when $(x^{(i)}, y^{(i)})$ form a *training example*, each $y^{(i)} \\in \\mathcal{Y}$ is a target. We call $\\mathcal{Y}$ the target space.\n",
    "\n",
    "- Binary (one-of-two) – Binary classification\n",
    "    - Sentiment: positive / negative\n",
    "    - Email: Spam / Not Spam\n",
    "    - Online Transactions: Fraudulent (Yes / No)\n",
    "    - Tumor: Malignant / Benign\n",
    "    - y ∈ 0,1 e.g. 0: Negative class, 1: Positive class\n",
    "    - y ∈ −1,1 e.g. -1: Negative class, 1: Positive class\n",
    "- **Multi-class (one-of-many, many-of-many problems) – Multi-class classification**\n",
    "    - Sentiment: Positive / negative / neutral\n",
    "    - Emotion: Happy, Sad, Surprised, Angry,...\n",
    "    - Part-of-Speech tag: Noun / verb / adjective / adverb /...\n",
    "    - Recognize a word: One of |V| tags\n",
    "    - y ∈ 0,1,2,3, ... e.g. 0: Happy, 1: Sad, 2, Angry,...\n",
    "- **Real-valued – Regression**\n",
    "    - Temperature, height, age, length, weight, duration, price...\n",
    "    \n",
    "**Regression vs. Classification**\n",
    "\n",
    "\n",
    "1. __Regression__: The target variable $y$ is continuous. We are fitting a curve in a high-dimensional feature space that approximates the shape of the dataset.\n",
    "2. __Classification__: The target variable $y$ is discrete. Each discrete value corresponds to a *class* and we are looking for a hyperplane that separates the different classes.\n",
    "\n",
    "<h3 align='center'>Feature Matrix</h3>\n",
    "\n",
    "Suppose that we have a dataset of size $n$ (e.g., $n$ patients), indexed by $i=1,2,...,n$. Each $x^{(i)}$ is a vector of $d$ features.\n",
    "\n",
    "Machine learning algorithms are most easily defined in the language of linear algebra. Therefore, it will be useful to represent the entire dataset as one matrix $X \\in \\mathbb{R}^{n \\times d}$, of the form:\n",
    "$$ X = \\begin{bmatrix}\n",
    "x^{(1)}_1 & x^{(2)}_1 & \\ldots & x^{(n)}_1 \\\\\n",
    "x^{(1)}_2 & x^{(2)}_2 & \\ldots & x^{(n)}_2 \\\\\n",
    "\\vdots \\\\\n",
    "x^{(1)}_d & x^{(2)}_d & \\ldots & x^{(n)}_d\n",
    "\\end{bmatrix}.$$\n",
    "\n",
    "Similarly, we can vectorize the target variables into a vector $y \\in \\mathbb{R}^n$ of the form\n",
    "$$ y = \\begin{bmatrix}\n",
    "y^{(1)} \\\\\n",
    "y^{(2)} \\\\\n",
    "\\vdots \\\\\n",
    "y^{(n)}\n",
    "\\end{bmatrix}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef7cd04",
   "metadata": {},
   "source": [
    "<h3 align=\"center\">Hypothesis Space</h3>\n",
    "\n",
    "- We call the set of possible functions or candidate models `the hypothesis class`.\n",
    "- The hypothesis $h$ is sampled from a hypothesis space $H$.\n",
    "$$h \\in H$$\n",
    "\n",
    "- $H$ can be thought of to contain classes of hypotheses which share sets of assumptions like\n",
    "    - Decisions tree\n",
    "    - Perceptron\n",
    "    - Neural networks\n",
    "    - Support Vector Machines\n",
    "    \n",
    "**Example:** $h \\in H$ for $H$: Decision trees, would be instances of\n",
    "decisions trees of different height, arity, thresholds, etc.\n",
    "\n",
    "- In machine learning, the `hypothesis space` is the set of all possible hypotheses that a learning algorithm can consider when making predictions. It is an important concept because the choice of hypotheses can significantly affect the performance of a learning algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "908967ba",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<function <lambda> at 0x7f74c0c24ee0>, <function <lambda> at 0x7f74c0c24e50>, <function <lambda> at 0x7f74c0c24f70>, <function <lambda> at 0x7f74c0c24670>, <function <lambda> at 0x7f74c134e1f0>, <function <lambda> at 0x7f74c134e040>, <function <lambda> at 0x7f74c134e0d0>, <function <lambda> at 0x7f74c134e160>, <function <lambda> at 0x7f74c134e310>, <function <lambda> at 0x7f74c134e3a0>, <function <lambda> at 0x7f74c134e430>, <function <lambda> at 0x7f74c134e4c0>, <function <lambda> at 0x7f74c134e550>, <function <lambda> at 0x7f74c134e5e0>, <function <lambda> at 0x7f74c134e670>, <function <lambda> at 0x7f74c134e700>, <function <lambda> at 0x7f74c134e790>, <function <lambda> at 0x7f74c134e820>, <function <lambda> at 0x7f74c134e8b0>, <function <lambda> at 0x7f74c134e940>, <function <lambda> at 0x7f74c134e9d0>, <function <lambda> at 0x7f74c134ea60>, <function <lambda> at 0x7f74c134eaf0>, <function <lambda> at 0x7f74c134eb80>, <function <lambda> at 0x7f74c134ec10>, <function <lambda> at 0x7f74c134eca0>, <function <lambda> at 0x7f74c134ed30>, <function <lambda> at 0x7f74c134edc0>, <function <lambda> at 0x7f74c134ee50>, <function <lambda> at 0x7f74c134eee0>, <function <lambda> at 0x7f74c134ef70>, <function <lambda> at 0x7f74c136a1f0>, <function <lambda> at 0x7f74c136a040>, <function <lambda> at 0x7f74c136a0d0>, <function <lambda> at 0x7f74c136a160>, <function <lambda> at 0x7f74c136a280>, <function <lambda> at 0x7f74c136a3a0>, <function <lambda> at 0x7f74c136a430>, <function <lambda> at 0x7f74c136a4c0>, <function <lambda> at 0x7f74c136a550>, <function <lambda> at 0x7f74c136a5e0>, <function <lambda> at 0x7f74c136a670>, <function <lambda> at 0x7f74c136a700>, <function <lambda> at 0x7f74c136a790>, <function <lambda> at 0x7f74c136a820>, <function <lambda> at 0x7f74c136a8b0>, <function <lambda> at 0x7f74c136a940>, <function <lambda> at 0x7f74c136a9d0>, <function <lambda> at 0x7f74c136aa60>, <function <lambda> at 0x7f74c136aaf0>, <function <lambda> at 0x7f74c136ab80>, <function <lambda> at 0x7f74c136ac10>, <function <lambda> at 0x7f74c136aca0>, <function <lambda> at 0x7f74c136ad30>, <function <lambda> at 0x7f74c136adc0>, <function <lambda> at 0x7f74c136ae50>, <function <lambda> at 0x7f74c136aee0>, <function <lambda> at 0x7f74c136af70>, <function <lambda> at 0x7f74c141ce50>, <function <lambda> at 0x7f74c141cca0>, <function <lambda> at 0x7f74c141c040>, <function <lambda> at 0x7f74c0c85280>, <function <lambda> at 0x7f74c0c85040>, <function <lambda> at 0x7f74c0c850d0>, <function <lambda> at 0x7f74c0c85160>, <function <lambda> at 0x7f74c0c851f0>, <function <lambda> at 0x7f74c0c85310>, <function <lambda> at 0x7f74c0c85430>, <function <lambda> at 0x7f74c0c854c0>, <function <lambda> at 0x7f74c0c85550>, <function <lambda> at 0x7f74c0c855e0>, <function <lambda> at 0x7f74c0c85670>, <function <lambda> at 0x7f74c0c85700>, <function <lambda> at 0x7f74c0c85790>, <function <lambda> at 0x7f74c0c85820>, <function <lambda> at 0x7f74c0c858b0>, <function <lambda> at 0x7f74c0c85940>, <function <lambda> at 0x7f74c0c859d0>, <function <lambda> at 0x7f74c0c85a60>, <function <lambda> at 0x7f74c0c85af0>, <function <lambda> at 0x7f74c0c85b80>, <function <lambda> at 0x7f74c0c85c10>, <function <lambda> at 0x7f74c0c85ca0>, <function <lambda> at 0x7f74c0c85d30>, <function <lambda> at 0x7f74c0c85dc0>, <function <lambda> at 0x7f74c0c85e50>, <function <lambda> at 0x7f74c0c85ee0>, <function <lambda> at 0x7f74c0c85f70>, <function <lambda> at 0x7f74c0cb6280>, <function <lambda> at 0x7f74c0cb6040>, <function <lambda> at 0x7f74c0cb60d0>, <function <lambda> at 0x7f74c0cb6160>, <function <lambda> at 0x7f74c0cb61f0>, <function <lambda> at 0x7f7452c7c8b0>, <function <lambda> at 0x7f7452c7c3a0>, <function <lambda> at 0x7f7452c883a0>, <function <lambda> at 0x7f7452c88310>, <function <lambda> at 0x7f7452c88280>, <function <lambda> at 0x7f7452c880d0>, <function <lambda> at 0x7f7452c88700>, <function <lambda> at 0x7f7452c88790>, <function <lambda> at 0x7f7452c88820>, <function <lambda> at 0x7f7452c888b0>, <function <lambda> at 0x7f7452c88940>, <function <lambda> at 0x7f7452c889d0>, <function <lambda> at 0x7f7452c88a60>, <function <lambda> at 0x7f7452c88af0>, <function <lambda> at 0x7f7452c88b80>, <function <lambda> at 0x7f7452c88c10>, <function <lambda> at 0x7f7452c88ca0>, <function <lambda> at 0x7f7452c88d30>, <function <lambda> at 0x7f7452c88dc0>, <function <lambda> at 0x7f7452c88e50>, <function <lambda> at 0x7f7452c88ee0>, <function <lambda> at 0x7f7452c88f70>, <function <lambda> at 0x7f74c1359040>, <function <lambda> at 0x7f74c13590d0>, <function <lambda> at 0x7f74c1359160>, <function <lambda> at 0x7f74c13591f0>, <function <lambda> at 0x7f74c1359280>, <function <lambda> at 0x7f74c1359310>]\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "\n",
    "# Define the input space X as a matrix of four rows and two columns, \n",
    "# representing four possible input examples with two features each.\n",
    "X = np.array([[0, 0], \n",
    "              [0, 1], \n",
    "              [1, 0], \n",
    "              [1, 1]])\n",
    "\n",
    "# Define the output space Y as a vector of four values, representing \n",
    "#  the corresponding output labels for each of the four input examples.\n",
    "Y = np.array([0, 1, 1, 0])\n",
    "\n",
    "# Define the hypothesis space H\n",
    "H = []\n",
    "\n",
    "# Iterate over all possible combinations of weights w1 and w2\n",
    "for w1 in range(-5, 6):\n",
    "    for w2 in range(-5, 6):\n",
    "        # Append the hypothesis h(x) = w1 * x1 + w2 * x2 to the hypothesis space H\n",
    "        result = lambda x: w1 * X[0] + w2 * X[1]\n",
    "        H.append(result)\n",
    "\n",
    "# Print the hypothesis space\n",
    "print(H)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3452a33b",
   "metadata": {},
   "source": [
    ">**Note:** The code prints the resulting `hypothesis space` $H$, which is a list of all possible hypotheses that can be considered by the learning algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb32a7a0",
   "metadata": {},
   "source": [
    "<h3><b><i>Question:</i></b> \n",
    "    For a given problem, How we can select/choose hypothesis(machine) $h \\in h$.</h3>\n",
    "    \n",
    "**Answer:**\n",
    "- **Randomly**\n",
    "    - May not work well\n",
    "    - Like using a random program to solve a sorting problem\n",
    "    - May work if $H$ is constrained enough\n",
    "\n",
    "- **Exhaustively**\n",
    "    - Would be very slow\n",
    "    - The space $H$ is usually very large (if not infinite)\n",
    "\n",
    "- $H$ is usually chosen by data scientists (you!) based on their experience!\n",
    "    - $h \\in H$ is estimated efficiently using various optimization techniques. Define hypothesis class $H$ for a given learning algorithm. Evaluate the performance of each candidate function and choose the best one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b4de7a",
   "metadata": {},
   "source": [
    "<h3><i>Question<i/>: How do we evaluate the performance? </h3>\n",
    "\n",
    "**Answer:** \n",
    "    \n",
    "Define a loss function to quantify/calculate the accuracy of the prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22de0490",
   "metadata": {},
   "source": [
    "<h3 align=\"center\">Loss Function</h3>\n",
    "\n",
    "- Loss function should quantify/calculate the average error in predicting $y$ using hypothesis function $h$ and input $x$. It is denoted by $L$.\n",
    "\n",
    "- Smaller is better\n",
    "    - 0 loss: No error\n",
    "    - 100% loss: Could not even get one instance right\n",
    "    - 50% loss: Your h is as informative as a coin toss\n",
    "    \n",
    "&emsp;    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aca097d",
   "metadata": {},
   "source": [
    "\n",
    "<h3 align=\"center\"><b>0/1 Loss</b></h3>\n",
    "\n",
    "- The `0/1 loss` is a loss function that is used to evaluate the performance of a machine learning model in binary classification tasks. It is defined as the number of incorrect predictions made by the model, divided by the total number of predictions.\n",
    "\n",
    "<img src=\"https://www.baeldung.com/wp-content/ql-cache/quicklatex.com-3fc482ec51a32e213970a07a3de41d10_l3.svg\" height=600px width=600px>\n",
    "\n",
    "- Counts the average number of mistakes in predicting $y$\n",
    "- Returns the training error rate\n",
    "- Not used due to Non-continuous and non-differentiable\n",
    "    - Difficult to utilize in optimization\n",
    "- Used to evaluate classifiers in binary/multiclass settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "673466a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "\n",
    "# Define the true labels y_true as a numpy array of four values, \n",
    "# representing the correct class labels for four input examples. \n",
    "y_true = np.array([0, 1, 0, 1])\n",
    "\n",
    "# Define the predicted labels y_pred as a numpy array of \n",
    "# four values, representing the class labels predicted by the model.\n",
    "y_pred = np.array([0, 0, 1, 1])\n",
    "\n",
    "# Calculate the 0/1 loss\n",
    "loss = np.mean(y_true != y_pred)\n",
    "\n",
    "# def zero_one_loss(y_true, y_pred):\n",
    "#     loss = 0\n",
    "#     for yt, yp in zip(y_true, y_pred):\n",
    "#         if yt != yp:\n",
    "#             loss += 1\n",
    "#     return loss\n",
    "\n",
    "# Print the loss\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72264972",
   "metadata": {},
   "source": [
    "> **Note:** The output will be `0.5`, indicating that the model made 2 incorrect predictions out of a total of 4, or 50% error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2866a04",
   "metadata": {},
   "source": [
    "<h3 align=\"center\"><b>Squared Loss Function</b></h3>\n",
    "\n",
    "The squared loss function, also known as the mean squared error (MSE) loss, is a common loss function used in regression tasks. It measures the average squared difference between the predicted values and the true values. The squared loss function is defined as:\n",
    "\n",
    "$$L_{sq}(h) = \\frac{1}{2n} \\sum_{i=1}^n \\left( h(x^{(i)}) - y^{(i)} \\right)^2$$\n",
    "\n",
    "Where $h(x^{(i)})$ is the predicted value, $y^{(i)}$ is the true value, and $n$ is the number of samples. These are defined for a dataset $\\mathcal{D} = \\{(x^{(i)}, y^{(i)}) \\mid i = 1,2,...,n\\}$.\n",
    "\n",
    "\n",
    "- Typically used in regression settings\n",
    "- The loss is always non-negative\n",
    "- The loss grows quadratically with the absolute magnitude of mis-prediction\n",
    "- Encourages no predictions to be really far off\n",
    "- If a prediction is very close to be correct, the square will be tiny and little attention will be given to that example to obtain zero error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f22d4220",
   "metadata": {},
   "outputs": [],
   "source": [
    "def squared_loss(y_true, y_pred):\n",
    "    loss = 0\n",
    "    for yt, yp in zip(y_true, y_pred):\n",
    "        loss += (yt - yp) ** 2\n",
    "    loss /= len(y_true)\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54177eb",
   "metadata": {},
   "source": [
    "**Note:** Function `squared_loss` takes in two arguments: `y_true` and `y_pred`. `y_true` is a list of true values, and `y_pred` is a list of predicted values. The function calculates the squared difference between each pair of true and predicted values and sums them up. It then divides the total loss by the number of samples to get the average loss. The function returns the average loss as a float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "13d8b9a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.65\n"
     ]
    }
   ],
   "source": [
    "y_true = [1, 2, 3, 4, 5]\n",
    "y_pred = [1.5, 2.5, 2.5, 4.5, 6.5]\n",
    "\n",
    "loss = squared_loss(y_true, y_pred)\n",
    "print(loss)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d0091b",
   "metadata": {},
   "source": [
    "<h3 align=\"center\"><b>Absolute Loss Function</b></h3>\n",
    "\n",
    "The absolute loss function, also known as the mean absolute error (MAE) loss, is another common loss function used in regression tasks. It measures the average absolute difference between the predicted values and the true values. The absolute loss function is defined as:\n",
    "\n",
    "$$L_{abs}(h) = \\frac{1}{n} \\sum_{i=1}^n \\left| h(x^{(i)}) - y^{(i)} \\right|$$\n",
    "\n",
    "Where $h(x^{(i)})$ is the predicted value, $y^{(i)}$ is the true value, and $n$ is the number of samples. These are defined for a dataset $\\mathcal{D} = \\{(x^{(i)}, y^{(i)}) \\mid i = 1,2,...,n\\}$.\n",
    "\n",
    "\n",
    "- The loss is always non-negative\n",
    "- The loss grows linearly with the absolute magnitude of mis-prediction\n",
    "- Better suited for noisy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ceb11696",
   "metadata": {},
   "outputs": [],
   "source": [
    "def absolute_loss(y_true, y_pred):\n",
    "    loss = 0\n",
    "    for yt, yp in zip(y_true, y_pred):\n",
    "        loss += abs(yt - yp)\n",
    "    loss /= len(y_true)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadf20f1",
   "metadata": {},
   "source": [
    "**Note:** The `absolute_loss` function takes in two arguments: `y_true` and `y_pred`. `y_true` is a list of true values, and `y_pred` is a list of predicted values. The function calculates the absolute difference between each pair of true and predicted values and sums them up. It then divides the total loss by the number of samples to get the average loss. The function returns the average loss as a float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "61ef8124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7\n"
     ]
    }
   ],
   "source": [
    "y_true = [1, 2, 3, 4, 5]\n",
    "y_pred = [1.5, 2.5, 2.5, 4.5, 6.5]\n",
    "\n",
    "loss = absolute_loss(y_true, y_pred)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a5e164",
   "metadata": {},
   "source": [
    "<h4 align=\"center\"><b>Comparsion</b></h4>\n",
    "<img src=\"images/p12.png\" align=\"left\">\n",
    "<img src=\"images/p13.png\" align=\"right\" height=400px width=400px>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a14a30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9ffb34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "106f08a0",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "**Regression:** Quantitative Prediction on a continuous scale.\n",
    "<img src=\"images/p11.png\">\n",
    "\n",
    "> Here, `PROCESS` or `SYSTEM` refers to any underlying physical or logical phenomenon which maps our input data to our observed and noisy output data.\n",
    "\n",
    "- **One Variable Regression:** $\\;\\;y$ is a scalar.\n",
    "- **Multi-Variable Regression:** $\\;\\;y$ is a vector.\n",
    "- **Single feature Regression:** $\\;\\;x$ is a scalar.\n",
    "- **Multiple feature Regression:** $\\;\\;x$ is a vector.\n",
    "\n",
    "<h3 align=\"center\"> Model Formulation and Setup</h3>\n",
    "\n",
    "#### *True Model:*\n",
    "We assume there is an inherent but unknown relationship between\n",
    "input and output.\n",
    "$y = f(x) + n$\n",
    "\n",
    "<img src=\"images/p16.png\" align=\"right\" height=400px width=400px>\n",
    "\n",
    "#### *Goal:* \n",
    "Given noisy observations, we need to estimate the unknown functional\n",
    "relationship as accurately as possible.\n",
    "\n",
    "We have:\n",
    "- For some input $x,\\hat y$ is our model output.\n",
    "- Assume that our model is $\\hat f(x,\\theta)$, characterized by the paramter(s) $\\theta$.\n",
    "- Model $f(x,\\theta)$ has\n",
    "    - A structure (e.g linear, polynomial, inverse).\n",
    "    - Parameters in the vector $\\theta = [\\theta_1,\\theta_2, \\theta_3,....,\\theta_M]$\n",
    "- Our Model error is $\\epsilon = y - \\hat y$.\n",
    "\n",
    "<img src=\"images/p17.png\" align=\"left\" height=400px width=400px>\n",
    "\n",
    "<img src=\"images/p18.png\" align=\"right\" height=500px width=500px>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e115be28",
   "metadata": {},
   "source": [
    "<h3 align=\"center\">Model</h3>\n",
    "\n",
    "We have :\n",
    "\n",
    "$$\\mathcal{D} = \\{(x^{(i)}, y^{(i)}) \\mid i = 1,2,...,n\\} \\in X^{R} \\times Y$$\n",
    "\n",
    "Model is a linear function of the features, that is \n",
    "\n",
    "$$\\hat f(X,\\theta) = \\theta_0 + \\sum_{i=1}^d(\\theta_ix_i) = \\theta_0 + \\theta^TX$$\n",
    "- Linear Structure\n",
    "- Model Parameters: $\\theta_0 \\;\\; and \\; \\theta = [\\theta_1,\\theta_2, \\theta_3,....,\\theta_d]$\n",
    "    - $\\theta_0$ is bias or intercept.\n",
    "    - $\\theta = [\\theta_1,\\theta_2, \\theta_3,....,\\theta_d]$ represents the weights or slope.\n",
    "    - $\\theta_i$ quanitfies the contribution of i-th feature $x_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10f3215",
   "metadata": {},
   "source": [
    "<h3 align=\"center\">What is Linear Model?</h3>\n",
    "\n",
    "#### When d = 1:\n",
    "$$\\hat f(X,\\theta) = \\theta_0 + \\theta_1x$$\n",
    "$$\\hat f(X,\\theta) = \\theta_0 + \\theta_1x$$\n",
    "- This represents the equation of `line`.\n",
    "<img src=\"images/p19.png\" align=\"right\">\n",
    "\n",
    "#### When d=2:\n",
    "$$\\hat f(X,\\theta) = \\theta_0 + \\theta_1x_1 + \\theta_2x_2$$\n",
    "- This represents the equation of the `Plane`\n",
    "<img src=\"images/p20.png\" align=\"right\">\n",
    "\n",
    "\n",
    "#### When d=d:\n",
    "$$\\hat f(X,\\theta) = \\theta_0 + \\theta_TX$$\n",
    "- This represents the `Hyper-plane` in $R^{d+1}$.\n",
    "\n",
    "----\n",
    "- For different $\\theta_0$ and , <b>$\\theta$</b>, we have differnt hyper-planes.\n",
    "- How do we find the `best` line?\n",
    "- What do we mean by the `best`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746dc641",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f1f282d4",
   "metadata": {},
   "source": [
    "<h3 align=\"center\">Linear Regression with one Variable </h3>\n",
    "\n",
    "**Notation:**\n",
    "\n",
    "- **m** = Number of training samples\n",
    "- **x** = Feature\n",
    "- **y** = Label\n",
    "- $(x^i , y^i)$: the ith sample in the dataset\n",
    "\n",
    "<img src=\"https://cdn.scribbr.com/wp-content/uploads//2020/02/simple-linear-regression-graph.png\" align=\"right\">\n",
    "<img src=\"images/21.png\" align=\"left\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fdde7a7",
   "metadata": {},
   "source": [
    "$$h_\\theta(x) = \\theta_0+ \\theta_1x$$\n",
    "\n",
    "> **Linear Regression with one variable is also called univarite linear regression, simple linear regression.**\n",
    "\n",
    "**Parameters:**\n",
    "$$\\theta_0, \\theta_1$$\n",
    "\n",
    "**Cost Function**\n",
    "$$J(\\theta_0, \\theta_1) = \\frac{1}{m} \\sum_{i=1}^m (h_\\theta(x^{(i)})-y^{(i)})^2$$\n",
    "\n",
    "**Goal:**\n",
    "$$minimum_{\\theta_0,\\theta_1} J(\\theta_0,\\theta_1)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "92a1b03c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def cost_function(y_pred, y):\n",
    "    n = len(y)\n",
    "    cost = 1/n * np.sum((y_pred - y)**2)\n",
    "    return cost\n",
    "y_pred = np.array([1, 2, 3])\n",
    "y = np.array([1, 2, 2])\n",
    "cost = cost_function(y_pred, y)\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d52de0e",
   "metadata": {},
   "source": [
    "<h3 align=\"center\">A simplified case</h3>\n",
    "\n",
    "<img src=\"images/21.jpeg\">\n",
    "<img src=\"images/22.jpeg\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca337a78",
   "metadata": {},
   "source": [
    "<h3 align=\"center\">Using both of the <i>knobs</i> </h3>\n",
    "\n",
    "**Hypothesis:**\n",
    "$$\\theta_0(x) = \\theta_0 + \\theta_1x$$\n",
    "\n",
    "**Parameters:**\n",
    "$$\\theta_0, \\theta_1$$\n",
    "\n",
    "**Cost Function**\n",
    "$$J(\\theta_0, \\theta_1) = \\frac{1}{m} \\sum_{i=1}^m (h_\\theta(x^{(i)})-y^{(i)})$$\n",
    "\n",
    "**Goal:**\n",
    "$$minimum_{\\theta_0,\\theta_1} J(\\theta_0,\\theta_1)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064d618c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "099b9204",
   "metadata": {},
   "source": [
    "<h3><i>Question:</i> How do we find the <i>best</i> line? What do we mean by <i>best</i>?</h3>\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "#### Define Loss Function:\n",
    ">Loss function should be a function of model parameters.\n",
    "\n",
    "- For input $x$, our model error is $e = y - \\hat y = y - \\hat f(x,\\theta) = y - \\theta_0 - \\theta^Tx$.\n",
    "- e is also termed as residual error as it is the differnce between observed value and predicted value.\n",
    "- **d=1**\n",
    "\n",
    "<img src=\"images/23.png\" align=\"center\">\n",
    "\n",
    "- For $\\mathcal{D} = \\{(x^{(i)}, y^{(i)}) \\mid i = 1,2,...,n\\} \\in X^{R} \\times Y$, we have\n",
    "$$e_i = y_i-\\theta_0-\\theta^T,\\;\\;\\;\\; i=1,2,3,4,.....,n$$\n",
    "\n",
    "- Using Residual error, we can define different loss functions:\n",
    "   $$L_{LSE}(\\theta_0,\\theta_1) = \\sum_{i=1}^n(y_i-\\theta_0-\\theta^Tx_i)^2$$\n",
    "   $$L_{MSE}(\\theta_0,\\theta_1) = {1/n} \\sum_{i=1}^n(y_i-\\theta_0-\\theta^Tx_i)^2$$\n",
    "   $$L_{MSE}(\\theta_0,\\theta_1) = \\sqrt {{1/n} \\sum_{i=1}^n(y_i-\\theta_0-\\theta^Tx_i)^2}$$\n",
    "   \n",
    "> One minimizer for all loss functions.\n",
    "\n",
    "- We minimize the following loss function:\n",
    "   $$L_{MSE}(\\theta_0,\\theta_1) = {1/n} \\sum_{i=1}^n(y_i-\\theta_0-\\theta^Tx_i)^2$$\n",
    "   \n",
    "- We have an **optimzation problem**: find the parameters which minimize the loss function. We write optimization problem (with no constraints) as \n",
    "   $$minimize_{\\theta_0,\\theta}\\;\\; L_{MSE}(\\theta_0,\\theta_1) = {1/n} \\sum_{i=1}^n(y_i-\\theta_0-\\theta^Tx_i)^2$$\n",
    "   \n",
    "   \n",
    "### How to solve?\n",
    "- **Analytically:** Determine a critical point that makes the derivtive(if it exists) equal to zero.\n",
    "- **Numerically:** Solve optimization using some algorithm that iteratively takes use closer to the critical point minimizing objective function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c686f9",
   "metadata": {},
   "source": [
    "## Example-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0144d5df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting gradient descent at b = 0, m = 0, error = 5565.107834483211\n",
      "Running...\n",
      "After 1000 iterations b = 0.08893651993741346, m = 1.4777440851894448, error = 112.61481011613473\n"
     ]
    }
   ],
   "source": [
    "from numpy import *\n",
    "\n",
    "# y = mx + b\n",
    "# m is slope, b is y-intercept\n",
    "def compute_error_for_line_given_points(b, m, points):\n",
    "    totalError = 0\n",
    "    for i in range(0, len(points)):\n",
    "        x = points[i, 0]\n",
    "        y = points[i, 1]\n",
    "        totalError += (y - (m * x + b)) ** 2\n",
    "    return totalError / float(len(points))\n",
    "\n",
    "def step_gradient(b_current, m_current, points, learningRate):\n",
    "    b_gradient = 0\n",
    "    m_gradient = 0\n",
    "    N = float(len(points))\n",
    "    for i in range(0, len(points)):\n",
    "        x = points[i, 0]\n",
    "        y = points[i, 1]\n",
    "        b_gradient += -(2/N) * (y - ((m_current * x) + b_current))\n",
    "        m_gradient += -(2/N) * x * (y - ((m_current * x) + b_current))\n",
    "    new_b = b_current - (learningRate * b_gradient)\n",
    "    new_m = m_current - (learningRate * m_gradient)\n",
    "    return [new_b, new_m]\n",
    "\n",
    "def gradient_descent_runner(points, starting_b, starting_m, learning_rate, num_iterations):\n",
    "    b = starting_b\n",
    "    m = starting_m\n",
    "    for i in range(num_iterations):\n",
    "        b, m = step_gradient(b, m, array(points), learning_rate)\n",
    "    return [b, m]\n",
    "\n",
    "def run():\n",
    "    points = genfromtxt(\"datasets/data.csv\", delimiter=\",\")\n",
    "    learning_rate = 0.0001\n",
    "    initial_b = 0 # initial y-intercept guess\n",
    "    initial_m = 0 # initial slope guess\n",
    "    num_iterations = 1000\n",
    "    print(\"Starting gradient descent at b = {0}, m = {1}, error = {2}\".format(initial_b, initial_m, compute_error_for_line_given_points(initial_b, initial_m, points)))\n",
    "    print(\"Running...\")\n",
    "    [b, m] = gradient_descent_runner(points, initial_b, initial_m, learning_rate, num_iterations)\n",
    "    print(\"After {0} iterations b = {1}, m = {2}, error = {3}\".format(num_iterations, b, m, compute_error_for_line_given_points(b, m, points)))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3a7a23",
   "metadata": {},
   "source": [
    "<img src=\"images/gradient_descent_example.gif\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd1cd0f",
   "metadata": {},
   "source": [
    "### Example - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4c21d5f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: Cost = 1.5427137296536395\n",
      "Iteration 10: Cost = 0.4055899449885261\n",
      "Iteration 20: Cost = 0.12820367641883942\n",
      "Iteration 30: Cost = 0.06040293217462261\n",
      "Iteration 40: Cost = 0.04382901547954687\n",
      "Iteration 50: Cost = 0.03977749736664156\n",
      "Iteration 60: Cost = 0.038787097644943355\n",
      "Iteration 70: Cost = 0.03854499293476682\n",
      "Iteration 80: Cost = 0.03848581007209732\n",
      "Iteration 90: Cost = 0.03847134273178296\n",
      "Final w: 1.9890302120562438\n",
      "Final b: 0.9697502088675694\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Generate some fake data for linear regression\n",
    "N = 100\n",
    "x = np.linspace(-1, 1, N)\n",
    "y = 2 * x + 1 + np.random.randn(N) * 0.2\n",
    "# Initialize weight and bias with random values\n",
    "w = np.random.randn()\n",
    "b = np.random.randn()\n",
    "# Set the learning rate\n",
    "alpha = 0.1\n",
    "# Set the number of iterations\n",
    "num_iterations = 100\n",
    "# Iterate through the gradient descent algorithm\n",
    "for i in range(num_iterations):\n",
    "    # Calculate the predicted values\n",
    "    y_pred = w * x + b\n",
    "    # Calculate the cost function\n",
    "    cost = 1/N * np.sum((y_pred - y) ** 2)\n",
    "    # Calculate the gradients\n",
    "    dw = 2/N * np.sum((y_pred - y) * x)\n",
    "    db = 2/N * np.sum(y_pred - y)\n",
    "    # Update the weights and biases\n",
    "    w = w - alpha * dw\n",
    "    b = b - alpha * db\n",
    "    # Print the cost every 10 iterations\n",
    "    if i % 10 == 0:\n",
    "        print(f'Iteration {i}: Cost = {cost}')\n",
    "# Print the final weights and biases\n",
    "print(f'Final w: {w}')\n",
    "print(f'Final b: {b}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5d6332",
   "metadata": {},
   "source": [
    "<h3 align=\"center\">Define Loss Function: Reformulation</h3>\n",
    "\n",
    "   $$L_{MSE}(\\theta_0,\\theta_1) = {1/n} \\sum_{i=1}^n(y_i-\\theta_0-\\theta^Tx_i)^2 = \\frac{1}{2}e^Te$$\n",
    "   \n",
    "**Explanation:**\n",
    "The transpose of a vector `v` is denoted by `v^T` and is defined as the `reflection` of `v` over the main diagonal of a matrix. For example, if `v` is a column vector:\n",
    "\n",
    "`\n",
    "v = [v1]\n",
    "    [v2]\n",
    "    [v3]\n",
    "`\n",
    "\n",
    "then the transpose of `v` is:\n",
    "\n",
    "`v^T = [v1 v2 v3]`\n",
    "\n",
    "The `dot product` of two vectors `u` and `v` is denoted by `u.v` and is defined as the sum of the products of the corresponding elements of the two vectors. For example, if u and v are column vectors:\n",
    "\n",
    "`\n",
    "u = [u1]\n",
    "    [u2]\n",
    "    [u3]\n",
    "v = [v1]\n",
    "    [v2]\n",
    "    [v3]`\n",
    "    \n",
    "then the dot product of u and v is:\n",
    "\n",
    "`u.v = u1*v1 + u2*v2 + u3*v3`\n",
    "\n",
    "Now, to prove that `e^T.e = n`, where `e` is an n-dimensional column vector with all elements equal to `1` and `n` is the number of elements in `e`, we can use the definition of the dot product:\n",
    "\n",
    "`e^T.e = e1*e1 + e2*e2 + ... + en*en\n",
    "       = 1*1 + 1*1 + ... + 1*1\n",
    "       = n`\n",
    "       \n",
    "Therefore, `e^T.e = n.`\n",
    "\n",
    "Note that this result holds for any n-dimensional column vector e with all elements equal to 1.\n",
    "\n",
    "\n",
    "   $$L_{MSE}(\\theta_0,\\theta_1) = {1/n} \\sum_{i=1}^n(y_i-\\theta_0-\\theta^Tx_i)^2 = \\frac{1}{2}e^Te$$\n",
    "  \n",
    "Here $e=[e_1,e_2,....,e_n]^T$ (column vector) where\n",
    "$$e_i = y_i-\\theta_0-\\theta^T,\\;\\;\\;\\; i=1,2,3,4,.....,n$$\n",
    "\n",
    "<img src=\"images/p26.png\" align=\"left\">\n",
    "<img src=\"images/p24.png\" align=\"right\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95951fa1",
   "metadata": {},
   "source": [
    "**Consequently:** \n",
    "\n",
    "\n",
    "Recall that we may fit a linear model by choosing $\\theta$ that minimizes the squared error:\n",
    "$$J(\\theta_0,\\theta)=\\frac{1}{2}\\sum_{i=1}^n(y_i-\\theta_0-\\theta^\\top x_i)^2 = \\frac{1}{2}e^Te$$\n",
    "We can write this sum in matrix-vector form as:\n",
    "$$J(\\theta_0,\\theta)=J(w) = \\frac{1}{2} (y-Xw)^\\top(y-Xw) = \\frac{1}{2} \\|y-Xw\\|^2,$$\n",
    "where $X$ is the design matrix and $\\|\\cdot\\|$ denotes the Euclidean norm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a1e7e6",
   "metadata": {},
   "source": [
    "### Solve Optimization Problem: (Analytical Solution employing Calculus)\n",
    "- very beautiful, elegant function we have here!\n",
    "\n",
    "We first write the loss function as\n",
    "<img src=\"images/p27.png\" align=\"center\">\n",
    "\n",
    "-  To further solve this, let us quickly talk about the concept of a gradient of a function.\n",
    "\n",
    "#### Gradient of a function: Overview\n",
    "- For a function $f(x)$ that maps $x \\; \\epsilon \\; R^d$ to $R$, we define a gradient (directional derivative) with respect to $x$ as.\n",
    "<img src=\"images/p28.png\" align=\"center\">\n",
    "\n",
    "- Derivative quantifies the rate of change along different directions.\n",
    "\n",
    "**Question:** Calculate $\\nabla$ of following functions.\n",
    "- $f(x) = a^Tx = x^Ta$\n",
    "- $f(x) = x^Tx$\n",
    "- $f(x) = x^TPx$\n",
    "\n",
    "\n",
    "\n",
    "**We have a loss function:**\n",
    "$$L(w) = \\frac{1}{2}(y^Ty - 2w^TX^Ty + w^TX^TXw)$$\n",
    "\n",
    "- Take gradient with respect to $w$ as\n",
    "\\begin{align*}\n",
    "\\nabla_w J(w) \n",
    "& = \\nabla_w \\frac{1}{2} (X w - y)^\\top  (X w - y) \\\\\n",
    "& = \\frac{1}{2} \\nabla_w \\left( (Xw)^\\top  (X w) - (X w)^\\top y - y^\\top (X w) + y^\\top y \\right) \\\\\n",
    "& = \\frac{1}{2} \\nabla_w \\left( w^\\top  (X^\\top X) w - 2(X w)^\\top y \\right) \\\\\n",
    "& = \\frac{1}{2} \\left( 2(X^\\top X) w - 2X^\\top y \\right) \\\\\n",
    "& = (X^\\top X) w - X^\\top y\n",
    "\\end{align*}\n",
    "\n",
    "We used the facts that $a^\\top b = b^\\top a$ (line 3), that $\\nabla_x b^\\top x = b$ (line 4), and that $\\nabla_x x^\\top A x = 2 A x$ for a symmetric matrix $A$ (line 4).\n",
    "\n",
    "> We know from calculus that a function is minimized when its derivative is set to zero. In our case, our objective function is a (multivariate) quadratic; hence it only has one minimum, which is the global minimum.\n",
    "\n",
    "- Setting the above derivative to zero, we obtain the *normal equations*:\n",
    "$$ (X^\\top X) w = X^\\top y.$$\n",
    "\n",
    "Hence, the value $w^*$ that minimizes this objective is given by:\n",
    "$$ w^* = (X^\\top X)^{-1} X^\\top y.$$\n",
    "\n",
    "\n",
    "- We have determined the weights for which LSE,MSE,RMSE or the norm of the residual is minimized.\n",
    "- This solution is referred to as least-squared solution as it minimizes the squared error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e747c30",
   "metadata": {},
   "source": [
    "<h2 align=\"center\">Gradient Descent Algorihtm</h2>\n",
    "\n",
    "**Goal:**   Minimize cost function $J(\\theta_0, \\theta_1)$.\n",
    "\n",
    "**Definition:** Used all over machine learning for minimization.\n",
    "\n",
    "**Problem:**\n",
    "- We have $J(\\theta_0, \\theta_1)$.\n",
    "- We want to get $min\\;J(\\theta_0, \\theta_1)$.\n",
    "\n",
    "**Solution**:\n",
    "- Start with some $J(\\theta_0,\\theta_1)$. For example $J(0,0)$.\n",
    "\n",
    "\n",
    "#### How does it work?\n",
    "- Start with initial guesse\n",
    "    - Start at 0,0 (or any other value)\n",
    "    - Keeping changing $\\theta_0$ and $\\theta_1$ a little bit to try and reduce $J(\\theta_0,\\theta_1)$.\n",
    "- Each time you change the parameters, you select the gradient which reduces J(θ0,θ1) the most possible \n",
    "- Repeat\n",
    "- Do so until you converge to a local minimum\n",
    "- Has an interesting property\n",
    "    - Where you start can determine which minimum you end up\n",
    "    - Here we can see one initialization point led to one local minimum\n",
    "    - The other led to a different one\n",
    "\n",
    "<img src=\"images/p29.png\" align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5993f004",
   "metadata": {},
   "source": [
    "<h3 align=\"center\">A simplified version of gradient descent</h3>\n",
    "\n",
    "Assume again that we set $\\theta_0 = 0$ and our hypothesis and cost function practically have only one coefficient, $\\theta_1$.\n",
    "   $$h_\\theta(x) = \\theta_1x$$\n",
    "\n",
    "repeat until convergence{"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "97a964c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The minimum value of the function is 8.3156 at x = 3.8375\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the function to be minimized\n",
    "def f(x):\n",
    "    return x**2 + 10*np.sin(x)\n",
    "\n",
    "# Define the gradient of the function\n",
    "def grad_f(x):\n",
    "    return 2*x + 10*np.cos(x)\n",
    "\n",
    "# Choose the step size (learning rate)\n",
    "alpha = 0.1\n",
    "\n",
    "# Set the initial value of x\n",
    "x = 5\n",
    "\n",
    "# Set the tolerance for the convergence criterion\n",
    "tol = 1e-6\n",
    "\n",
    "# Initialize a list to store the values of x at each iteration\n",
    "x_values = [x]\n",
    "\n",
    "# Iterate until convergence\n",
    "while True:\n",
    "    # Compute the gradient at the current value of x\n",
    "    grad = grad_f(x)\n",
    "    \n",
    "    # Update the value of x using gradient descent\n",
    "    x = x - alpha * grad\n",
    "    \n",
    "    # Store the new value of x\n",
    "    x_values.append(x)\n",
    "    \n",
    "    # Check for convergence\n",
    "    if np.abs(grad) < tol:\n",
    "        break\n",
    "\n",
    "# Print the minimum value found\n",
    "print(f\"The minimum value of the function is {f(x):.4f} at x = {x:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e03e445",
   "metadata": {},
   "source": [
    "In this example, the function f(x) is defined as x**2 + 10*np.sin(x), and the gradient of the function, grad_f(x), is defined as 2*x + 10*np.cos(x). The step size (learning rate) is chosen as alpha = 0.1, and the initial value of x is set to 5. The tolerance for the convergence criterion is set to tol = 1e-6, which means that the algorithm will stop when the absolute value of the gradient falls below this threshold. The algorithm iteratively updates the value of x using the gradient descent rule x = x - alpha * grad, and stores the values of x at each iteration in the list x_values. The loop terminates when the absolute value of the gradient falls below the tolerance threshold, at which point the minimum value of the function has been found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4caefb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd37c59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bd4e30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ed98f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf5b425",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4495a6a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0d5796",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7367f44d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4d70df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8657076",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f72ee88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685fcabc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51c9a44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538096ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf0bf09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d6079a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66cd6c79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c72dc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b6593b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb32727c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c154247",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7619cf0a",
   "metadata": {},
   "source": [
    "# Deriving the Gradient Descent Algorithm for Linear Regression\n",
    "\n",
    "Linear regression is a supervised learning algorithm that is used to predict a continuous target variable given a set of input features. The goal of linear regression is to find the best model parameters (i.e., the weights and bias) that minimize the error between the predicted value and the true value of the target variable.\n",
    "\n",
    "One way to find the best model parameters is to use the gradient descent algorithm. In this notebook, we will derive the gradient descent algorithm for linear regression.\n",
    "\n",
    "## Linear Regression Model\n",
    "\n",
    "In linear regression, we assume that the relationship between the input features and the target variable is linear. This can be expressed as:\n",
    "\n",
    "$$ \\hat{y} = w_1 x_1 + w_2 x_2 + \\dots + w_n x_n + b $$\n",
    "\n",
    "where $\\hat{y}$ is the predicted value, $x_1, x_2, \\dots, x_n$ are the input features, $w_1, w_2, \\dots, w_n$ are the model weights, and $b$ is the bias term.\n",
    "\n",
    "## Loss Function\n",
    "\n",
    "To measure the error between the predicted value and the true value of the target variable, we can use a loss function. One common loss function is the mean squared error (MSE) loss, which is defined as:\n",
    "\n",
    "$$ L = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y_i})^2 $$\n",
    "\n",
    "where $n$ is the number of samples, $y_i$ is the true value of the target variable, and $\\hat{y_i}$ is the predicted value.\n",
    "\n",
    "## Gradient Descent\n",
    "\n",
    "To minimize the MSE loss, we can use the gradient descent algorithm. The gradient descent algorithm works by iteratively updating the model weights and bias to reduce the loss.\n",
    "\n",
    "The update rule for the weights is:\n",
    "\n",
    "$$ w_j = w_j - \\alpha \\frac{\\partial L}{\\partial w_j} $$\n",
    "\n",
    "where $\\alpha$ is the learning rate and $\\frac{\\partial L}{\\partial w_j}$ is the partial derivative of the loss with respect to $w_j$.\n",
    "\n",
    "The update rule for the bias is:\n",
    "\n",
    "$$ b = b - \\alpha \\frac{\\partial L}{\\partial b} $$\n",
    "\n",
    "where $\\alpha$ is the learning rate and $\\frac{\\partial L}{\\partial b}$ is the partial derivative of the loss with respect to $b$.\n",
    "\n",
    "To compute the partial derivatives, we can use the chain rule:\n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial w_j} = \\frac{\\partial L}{\\partial \\hat{y}} \\frac{\\partial \\hat{y}}{\\partial w_j} $$\n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial b} = \\frac{\\partial L}{\\partial \\hat{y}} \\frac{\\partial \\hat{y}}{\\partial b} $$\n",
    "\n",
    "Substituting the expressions for the loss and the linear regression model, we get:\n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial w_j} = \\frac{2}{n} \\sum_{i=1}^{n} (y_i - \\hat{y_i}) (-x_{i,j}) $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab811322",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "53bd8175",
   "metadata": {},
   "source": [
    "# Deriving the Batch Gradient Descent Algorithm for Linear Regression\n",
    "\n",
    "Linear regression is a supervised learning algorithm that is used to predict a continuous target variable given a set of input features. The goal of linear regression is to find the best model parameters (i.e., the weights and bias) that minimize the error between the predicted value and the true value of the target variable.\n",
    "\n",
    "One way to find the best model parameters is to use the batch gradient descent algorithm. In this notebook, we will derive the batch gradient descent algorithm for linear regression.\n",
    "\n",
    "## Linear Regression Model\n",
    "\n",
    "In linear regression, we assume that the relationship between the input features and the target variable is linear. This can be expressed as:\n",
    "\n",
    "$$ \\hat{y} = w_1 x_1 + w_2 x_2 + \\dots + w_n x_n + b $$\n",
    "\n",
    "where $\\hat{y}$ is the predicted value, $x_1, x_2, \\dots, x_n$ are the input features, $w_1, w_2, \\dots, w_n$ are the model weights, and $b$ is the bias term.\n",
    "\n",
    "## Loss Function\n",
    "\n",
    "To measure the error between the predicted value and the true value of the target variable, we can use a loss function. One common loss function is the mean squared error (MSE) loss, which is defined as:\n",
    "\n",
    "$$ L = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y_i})^2 $$\n",
    "\n",
    "where $n$ is the number of samples, $y_i$ is the true value of the target variable, and $\\hat{y_i}$ is the predicted value.\n",
    "\n",
    "## Gradient Descent\n",
    "\n",
    "To minimize the MSE loss, we can use the batch gradient descent algorithm. The batch gradient descent algorithm works by iteratively updating the model weights and bias to reduce the loss.\n",
    "\n",
    "The update rule for the weights is:\n",
    "\n",
    "$$ w_j = w_j - \\alpha \\frac{\\partial L}{\\partial w_j} $$\n",
    "\n",
    "where $\\alpha$ is the learning rate and $\\frac{\\partial L}{\\partial w_j}$ is the partial derivative of the loss with respect to $w_j$.\n",
    "\n",
    "The update rule for the bias is:\n",
    "\n",
    "$$ b = b - \\alpha \\frac{\\partial L}{\\partial b} $$\n",
    "\n",
    "where $\\alpha$ is the learning rate and $\\frac{\\partial L}{\\partial b}$ is the partial derivative of the loss with respect to $b$.\n",
    "\n",
    "To compute the partial derivatives, we can use the chain rule:\n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial w_j} = \\frac{\\partial L}{\\partial \\hat{y}} \\frac{\\partial \\hat{y}}{\\partial w_j} $$\n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial b} = \\frac{\\partial L}{\\partial \\hat{y}} \\frac{\\partial \\hat{y}}{\\partial b} $$\n",
    "\n",
    "Substituting the expressions for the loss and the linear regression model, we get:\n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial w_j} = \\frac{2}{n} \\sum_{i=1}^{n} (y_i - \\hat{y_i}) (-x_{i,j}) $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66407fbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5ba7d624",
   "metadata": {},
   "source": [
    "Installing Python and a code editor (e.g., PyCharm)\n",
    "Basics of Python syntax and data types (e.g., variables, strings, integers, lists)\n",
    "Loops and conditional statements\n",
    "Functions and modules\n",
    "Object-oriented programming\n",
    "Built-in data structures (e.g., lists, dictionaries, sets)\n",
    "Input/output (I/O) functions\n",
    "Error handling\n",
    "Regular expressions\n",
    "Built-in math functions and modules (e.g., math, statistics)\n",
    "Built-in datetime module\n",
    "Built-in os and sys modules\n",
    "Parsing and generating JSON data\n",
    "Parsing and generating XML data\n",
    "Working with ZIP files\n",
    "Built-in collections module\n",
    "Built-in itertools module\n",
    "Multiprocessing\n",
    "Threading\n",
    "Executing external processes\n",
    "Working with databases (e.g., SQLite, MySQL)\n",
    "Web scraping\n",
    "Working with web APIs\n",
    "Working with data visualization libraries (e.g., Matplotlib, Seaborn)\n",
    "Machine learning with scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0280082e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c15f4f4d",
   "metadata": {},
   "source": [
    "# Deriving the Sigmoid Function for Logistic Regression\n",
    "\n",
    "Logistic regression is a supervised learning algorithm that is used to predict a binary target variable given a set of input features. The goal of logistic regression is to find the best model parameters (i.e., the weights and bias) that maximize the probability of the target variable being 1.\n",
    "\n",
    "To represent the probability of the target variable being 1, we can use the sigmoid function. The sigmoid function is defined as:\n",
    "\n",
    "$$ \\sigma(x) = \\frac{1}{1 + e^{-x}} $$\n",
    "\n",
    "where $x$ is the input to the function.\n",
    "\n",
    "The output of the sigmoid function is always between 0 and 1, which makes it suitable for representing probabilities. For example, if the output of the sigmoid function is 0.8, we can interpret this as an 80% probability of the target variable being 1.\n",
    "\n",
    "In logistic regression, we use the sigmoid function to transform the linear combination of the input features and the model parameters into a probability. This can be expressed as:\n",
    "\n",
    "$$ \\hat{y} = \\sigma(w_1 x_1 + w_2 x_2 + \\dots + w_n x_n + b) $$\n",
    "\n",
    "where $\\hat{y}$ is the predicted probability, $x_1, x_2, \\dots, x_n$ are the input features, $w_1, w_2, \\dots, w_n$ are the model weights, and $b$ is the bias term.\n",
    "\n",
    "To find the best model parameters, we can use an optimization algorithm (e.g., gradient descent) to maximize the likelihood of the target variable being 1.\n",
    "\n",
    "## Visualizing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f579ccd",
   "metadata": {},
   "source": [
    "# Deriving the Sigmoid Function for Logistic Regression\n",
    "\n",
    "Logistic regression is a supervised learning algorithm that is used for classification tasks. It is based on the idea of using a linear model to predict the probability of a binary outcome (i.e., 0 or 1).\n",
    "\n",
    "To map the predicted probability to the range 0 to 1, we can use a sigmoid function. The sigmoid function is defined as:\n",
    "\n",
    "$$ \\sigma(z) = \\frac{1}{1 + e^{-z}} $$\n",
    "\n",
    "where $z$ is the input to the function.\n",
    "\n",
    "The sigmoid function has the following properties:\n",
    "\n",
    "- It maps any real-valued number to the range 0 to 1\n",
    "- It has an \"S\" shape, with a smooth curve that approaches 0 as the input becomes more negative, and approaches 1 as the input becomes more positive\n",
    "- It is differentiable, which makes it suitable for use in gradient descent algorithms\n",
    "\n",
    "The sigmoid function can be used in logistic regression by taking the dot product of the input features and the model weights, and passing the result through the sigmoid function:\n",
    "\n",
    "$$ \\hat{y} = \\sigma(\\mathbf{w}^T \\mathbf{x}) $$\n",
    "\n",
    "where $\\hat{y}$ is the predicted probability, $\\mathbf{w}$ is the vector of model weights, and $\\mathbf{x}$ is the vector of input features.\n",
    "\n",
    "To train the model, we can use a loss function such as the cross-entropy loss to measure the difference between the predicted probability and the true label. The model parameters can then be updated using a gradient descent algorithm to minimize the loss.\n",
    "\n",
    "By using the sigmoid function, logistic regression is able to predict probabilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63574f8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "da661625",
   "metadata": {},
   "source": [
    "Sure! Here is a pseudocode for the gradient descent algorithm:\n",
    "\n",
    "- Initialize the parameters of the model, such as the weights and biases, with random values.\n",
    "- Calculate the cost function (also called the loss function) for the current values of the parameters.\n",
    "- Calculate the gradient of the cost function with respect to the parameters.\n",
    "- Update the parameters using the gradient and a learning rate alpha: \n",
    "    $$ parameter = parameter - alpha * gradient $$\n",
    "- Repeat steps 2-4 until the cost function reaches a minimum or the maximum number of iterations is reached.\n",
    "\n",
    "\n",
    "Here is a mathematical example of the gradient descent algorithm for a simple linear regression model with one weight w and one bias b:\n",
    "\n",
    "- Initialize w and b with random values.\n",
    "- Calculate the cost function: \n",
    "$$ J(w,b) = fact{1/N} * sum( (y_pred - y)^2 ) $$\n",
    "where $$ y_pred = w * x + b $$ and N is the number of data points.\n",
    "- Calculate the gradient of the cost function with respect to w and b:\n",
    "    $$ dw = 2/N * sum( (y_{pred} - y) * x )$$\n",
    "    $$ db = 2/N * sum( (y_{pred} - y) ) $$\n",
    "- Update the parameters using the gradient and a learning rate alpha:\n",
    "    $$ w = w - alpha * dw $$\n",
    "    $$ b = b - alpha * db $$\n",
    "- Repeat steps 2-4 until the cost function reaches a minimum or the maximum number of iterations is reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07f6e53f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: Cost = 4.544354874302624\n",
      "Iteration 10: Cost = 0.2711390881537866\n",
      "Iteration 20: Cost = 0.08401917395160328\n",
      "Iteration 30: Cost = 0.04816343170598533\n",
      "Iteration 40: Cost = 0.03951241913418109\n",
      "Iteration 50: Cost = 0.037398980098625904\n",
      "Iteration 60: Cost = 0.03688236187932412\n",
      "Iteration 70: Cost = 0.03675607394872026\n",
      "Iteration 80: Cost = 0.036725202675043214\n",
      "Iteration 90: Cost = 0.03671765614551247\n",
      "Final w: 1.9616102370712132\n",
      "Final b: 0.9940260130434699\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Generate some fake data for linear regression\n",
    "N = 100\n",
    "x = np.linspace(-1, 1, N)\n",
    "y = 2 * x + 1 + np.random.randn(N) * 0.2\n",
    "# Initialize weight and bias with random values\n",
    "w = np.random.randn()\n",
    "b = np.random.randn()\n",
    "# Set the learning rate\n",
    "alpha = 0.1\n",
    "# Set the number of iterations\n",
    "num_iterations = 100\n",
    "# Iterate through the gradient descent algorithm\n",
    "for i in range(num_iterations):\n",
    "    # Calculate the predicted values\n",
    "    y_pred = w * x + b\n",
    "    # Calculate the cost function\n",
    "    cost = 1/N * np.sum((y_pred - y) ** 2)\n",
    "    # Calculate the gradients\n",
    "    dw = 2/N * np.sum((y_pred - y) * x)\n",
    "    db = 2/N * np.sum(y_pred - y)\n",
    "    # Update the weights and biases\n",
    "    w = w - alpha * dw\n",
    "    b = b - alpha * db\n",
    "    # Print the cost every 10 iterations\n",
    "    if i % 10 == 0:\n",
    "        print(f'Iteration {i}: Cost = {cost}')\n",
    "# Print the final weights and biases\n",
    "print(f'Final w: {w}')\n",
    "print(f'Final b: {b}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b66a06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3687bff0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b980eecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d8f87f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215b275d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "16887c94",
   "metadata": {},
   "source": [
    "### Interview questions of Linear regression in machine learning\n",
    "Here are some common interview questions on linear regression in machine learning:\n",
    "- What is linear regression and how does it work?\n",
    "- What is the difference between simple linear regression and multiple linear regression?\n",
    "- How do you evaluate the performance of a linear regression model?\n",
    "- What is the ordinary least squares (OLS) method and how is it used in linear regression?\n",
    "- How can you prevent overfitting in linear regression?\n",
    "- Can you give an example of how linear regression can be used in a real-world problem?\n",
    "- How do you decide which variables to include in a linear regression model?\n",
    "- How do you handle collinearity in a linear regression model?\n",
    "- What are some common assumptions made in linear regression?\n",
    "- Can you discuss the limitations of linear regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d556d76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
