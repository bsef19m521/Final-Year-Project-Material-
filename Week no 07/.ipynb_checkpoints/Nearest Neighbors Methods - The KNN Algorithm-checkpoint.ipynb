{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b525179",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Nearest Neighbors Methods - The KNN Algorithm</h1>\n",
    "\n",
    "### KNN Algorithm\n",
    "- Basic Idea\n",
    "- Formal Definition\n",
    "- KNN Decision Boundary\n",
    "- A supervised, non-parametric algorithm\n",
    "- Used for classification and regression\n",
    "- An Instance-based learning algorithm\n",
    "- A lazy learning algorithm\n",
    "- Characteristics of kNN\n",
    "- Practical issues\n",
    "\n",
    "### Similarity/Distance Metrics \n",
    "- Constraints/Properties on Distance Metrics\n",
    "- Euclidean Distance\n",
    "- Manhatten Distance\n",
    "- Minkowski distance\n",
    "- Chebyshev Distance\n",
    "- Norm of a vector and Its Properties\n",
    "- Cosine Distance \n",
    "- Practical Issues in computing distance\n",
    "\n",
    "### The KNN algorithm and Implementation\n",
    "- KNN regression and classification with examples\n",
    "- Space and Time complexity\n",
    "- Choosing the value of K - The Theory.\n",
    "- Tuning the hyperparameter K - The method\n",
    "- KNN: The good, the bad and the ugly\n",
    "    \n",
    "### Algorithm Convergence\n",
    "- Error Convergence\n",
    "- Learning Problem\n",
    "- Bayes Optimal Classifier\n",
    "- 1-NN Error as n → ∞\n",
    "\n",
    "\n",
    "### KNN Enhancements\n",
    "- Parzen Windows and Kernels (Fast KNN) \n",
    "- Performance of KNN Algorithm\n",
    "- K-D Trees\n",
    "- Locality-sensitive Hashing\n",
    "- Inverted Lists\n",
    "\n",
    "### The Curse of Dimensionality\n",
    "- KNN Assumption\n",
    "- Demonstration\n",
    "- How does KNN work at all?\n",
    "\n",
    "### Dimensionality Reduction(Optional)\n",
    "- Why? and Benefits.\n",
    "- Difference between Feature Selection and Feature Extraction\n",
    "- Feature Selection methods\n",
    "- Feature Extraction\n",
    "- Principal Component Analysis\n",
    "    - Geometric Intuition\n",
    "    - Mathematical Formulation\n",
    "    - How do we choose K?\n",
    "    - Practical Consideration and Limitations\n",
    "\n",
    "### Model Evaluation Techniques\n",
    "- Classification Accuracy (0/1 Loss)\n",
    "- TP, TN, FP and FN\n",
    "- Confusion Matrix\n",
    "- Sensitivity, Specificity, Precision Trade-offs, ROC, AUC\n",
    "- F1-Score and Matthew’s Correlation Coefficient\n",
    "- Multi-class Classification, Evaluation, Micro, Macro Averaging\n",
    "\n",
    "**KNN: Python Implementation**    \n",
    "**KNN: Scikit-learn implementation**    \n",
    "**Interview Questions.**   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5df2cd7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
